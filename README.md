# üöÄ LLM Inference Optimization with TensorRT + Triton

> **End-to-end deployment of Transformer-based LLMs with INT8 quantization, CUDA-level profiling, and production-grade Triton serving pipeline.**

---

## üìå Overview

This project showcases a production-oriented LLM inference pipeline, integrating key optimizations used by real-world teams at NVIDIA, Meta, Amazon, and OpenAI. It includes:

- ‚úÖ Hugging Face ‚Üí ONNX model export with dynamic axis support
- ‚úÖ INT8 quantization via QDQ format and calibration dataset
- ‚úÖ TensorRT engine build + performance benchmarking
- ‚úÖ Kernel-level analysis with Nsight Systems (nsys) and NVTX annotations
- ‚úÖ Scalable inference serving with Triton Inference Server
- ‚úÖ Real throughput and latency benchmarking using perf_analyzer

> üéØ **Objective**: Maximize inference speed while preserving accuracy and enabling deployment scalability.

---

## üí° Why This Project Matters

Modern LLM inference faces growing challenges: high latency, memory bottlenecks, and limited scalability. This project addresses these by:

- Demonstrating a full pipeline: PyTorch ‚Üí ONNX ‚Üí TensorRT ‚Üí Triton
- Diagnosing and mitigating quantization-induced accuracy drops
- Using Nsight Systems + NVTX for deep CUDA visibility
- Leveraging Triton's batching and concurrent stream support for deployment

> ‚ö°Ô∏è A hands-on, real-world implementation of the techniques powering today‚Äôs most efficient AI systems.

---

## üîß Key Features

### ‚ú≥Ô∏è  ONNX Export + INT8 Quantization
- Model exported with dynamic shapes via optimum.exporters.onnx
- INT8 quantization using QDQ + calibration dataset
- Layer-level override to mitigate quality degradation

### ‚ú≥Ô∏è  TensorRT Engine Optimization
- Uses both trtexec and Python API for engine builds
- Compares FP32 / FP16 / INT8 performance
- Workspace tuning, layer fusion, and dynamic batch support

### ‚ú≥Ô∏è  CUDA Profiling & Bottleneck Detection
- Deep dive via nsys timeline + NVTX annotations
- Identifies:
  - cudaMemcpyAsync latency
  - Kernel launch delays
  - Underutilized GPU streams

### ‚ú≥Ô∏è  Triton Inference Server Deployment
- Configured model_repository for dynamic input, batching
- Python client for benchmarking
- Production-style API interface (HTTP/gRPC)

---

## üß± Architecture Diagram

```plaintext
[ Hugging Face Model ]
          ‚Üì (export)
      [ ONNX Format ]
          ‚Üì (optimize)
    [ TensorRT Engine ]
          ‚Üì (serve)
[ Triton Inference Server ]
          ‚Üì (client)
    [ HTTP/gRPC Inference ]
```

> Each step is profiled, visualized, and tuned using NVIDIA‚Äôs recommended tooling.

---

## üìÅ Project Structure

```bash
convert/
    export_onnx.py            # Hugging Face ‚Üí ONNX
    quantize_model.py         # QDQ quantization
optimize/
    build_trt_engine.py       # FP16/INT8 engine conversion
    trtexec_command.sh
serve/
    model_repository/         # Triton model setup
    run_triton.sh             # Launch Triton Server
    client/infer_client.py    # Python client for inference
profile/
    run_nsys_profile.py       # NVTX + Nsight Systems profiling
notebooks/
    latency_analysis.ipynb    # Performance plots
diagrams/
    pipeline_architecture.png # Inference system diagram
```

---

## üöÄ Quickstart

```bash
# 1. Export model to ONNX
python convert/export_onnx.py

# 2. Apply INT8 Quantization (optional)
python convert/quantize_model.py

# 3. Build Engine with TensorRT
bash optimize/trtexec_command.sh

# 4. Launch Triton Server
bash serve/run_triton.sh

# 5. Run Inference via Client
python serve/client/infer_client.py

# 6. Run Nsight Profiling
python profile/run_nsys_profile.py
```

---

## üìâ Results (In Progress)

| Engine Type      | Latency (ms) | Throughput (req/s) |
|------------------|--------------|--------------------|
| PyTorch Baseline | TBD          | TBD                |
| ONNX (FP32)      | TBD          | TBD                |
| TensorRT (FP16)  | TBD          | TBD                |
| TensorRT (INT8)  | TBD          | TBD                |

> Final benchmarks will include token throughput, memory footprint, and profiling overlays.

---

## Insights & Debug Notes

- NVTX tracing revealed kernel-streaming bottlenecks in memory ops

- INT8 quality loss was mitigated by per-layer selective dequantization

- Triton's dynamic batching is expected to improve throughput by ~40% without latency cost

- PyTorch to ONNX export required careful handling of dynamic axes

---

## üß† Key Takeaways

- Understanding where latency comes from (e.g., memcpy, shape inference, fallback to FP32) is key to optimizing inference.

- CUDA profiling with Nsight is non-optional when deploying real-time LLMs.

- Triton makes scalable, production-grade LLM inference practical and maintainable.

---

## ü§ù Open to Connect

If you're building high-performance LLM inference infra or interested in discussing deployment tools, let's connect.

üíª **GitHub**: [github.com/eric-wong-llm](https://github.com/eric-wong-llm)
üîó Open to feedback and collaboration!

---

## üîó References

- [TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/)
- [Triton Inference Server](https://github.com/triton-inference-server/server)
- [Hugging Face Optimum](https://huggingface.co/docs/optimum/index)
- [Nsight Systems](https://developer.nvidia.com/nsight-systems)
